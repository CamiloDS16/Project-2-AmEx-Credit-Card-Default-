{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployment to Production Data:\n",
    "\n",
    "This notebook aims to simulate the deplyoment of the XGB model built to production data. Please note that the dataset is an unseen test set provided by the data masters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load the data and drop Unnamed column.\n",
    "    \n",
    "    Parameters:\n",
    "    path where the data is stored.\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(path)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# load pkl files for model, imputer, scaler  \n",
    "def load_assets(model_path, imputer_path, scaler_path):\n",
    "    \"\"\"\n",
    "    Load the model, imputer, and scaler from disk.\n",
    "    \"\"\"\n",
    "    model = joblib.load(model_path)\n",
    "    imputer = joblib.load(imputer_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    return model, imputer, scaler\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Function to clean data by handling missing values and anomalies.\n",
    "    \"\"\"\n",
    "    # renaming columns\n",
    "    df = df.rename(columns={'credit_limit_used(%)': 'credit_limit_used_pctg'})\n",
    "    # Filling missing values for 'no_of_children', 'owns_car', 'migrant_worker', 'total_family_members' with mode\n",
    "    for col in ['no_of_children', 'owns_car', 'migrant_worker', 'total_family_members']:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "    # Handling 'XNA' values in 'gender'\n",
    "    df['gender'] = df['gender'].replace('XNA', df['gender'].mode()[0])\n",
    "\n",
    "    # Your data might have specific anomalies that you discovered during EDA.\n",
    "    # Include code to handle those anomalies here.\n",
    "    \n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Function for feature engineering, like combining certain features for new insights.\n",
    "    \"\"\"\n",
    "    # dropping columns to mitigate multicollinearity\n",
    "    df = df.drop(columns=['name', 'credit_limit', 'no_of_children'], axis=1)\n",
    "    # combining 'prev_defaults' and 'default_in_last_6months' into 'total_defaults'\n",
    "    if 'prev_defaults' in df.columns and 'default_in_last_6months' in df.columns:\n",
    "        df['total_defaults'] = df['prev_defaults'] + df['default_in_last_6months']\n",
    "        df = df.drop(columns=['prev_defaults', 'default_in_last_6months'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_data(df, imputer, scaler, original_features):\n",
    "    \"\"\"\n",
    "    Function to perform necessary data transformations like encoding and scaling.\n",
    "    \"\"\"\n",
    "    # Encoding categorical variables and scaling\n",
    "    df['gender'] = df['gender'].map({'F': 0, 'M': 1})\n",
    "    df['owns_car'] = df['owns_car'].map({'N': 0, 'Y': 1})\n",
    "    df['owns_house'] = df['owns_house'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "    # One-hot encoding for 'occupation_type'\n",
    "    df = pd.get_dummies(df, columns=['occupation_type'], prefix=['ot'])\n",
    "    # imputting missing values\n",
    "    df = pd.DataFrame(imputer.transform(df), columns=df.columns)\n",
    "    # Scaling the dataset\n",
    "    df = pd.DataFrame(scaler.transform(df), columns=df.columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def formatting(df):\n",
    "    # Assuming 'customer_id' is not a feature used in the model, \n",
    "    # but an identifier that you'll want to keep for later reference.\n",
    "    if 'customer_id' in df.columns:\n",
    "        customer_ids = df['customer_id']\n",
    "        X = df.drop(columns=['customer_id'], axis=1)\n",
    "    else:\n",
    "        customer_ids = None\n",
    "        X = df\n",
    "    \n",
    "    return X, customer_ids\n",
    "\n",
    "def prepare_for_prediction(X, model):\n",
    "    \"\"\"\n",
    "    Ensure the order of columns in the new data (X) matches the features used for training the model.\n",
    "    If there's a mismatch, reorder columns in X to match the training features.\n",
    "    \"\"\"\n",
    "    expected_features = model.get_booster().feature_names\n",
    "    received_features = list(X.columns)\n",
    "\n",
    "    # Check if all expected features are present\n",
    "    if not set(expected_features).issubset(received_features):\n",
    "        missing_features = set(expected_features) - set(received_features)\n",
    "        raise ValueError(f\"Missing features: {missing_features}\")\n",
    "\n",
    "    # If features are present but in different order, reorder them\n",
    "    if expected_features != received_features:\n",
    "        X = X[expected_features]\n",
    "    \n",
    "    return X\n",
    "\n",
    "# predictions\n",
    "def make_predictions(X, model):\n",
    "    \"\"\"\n",
    "    Make predictions, and return results.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X)\n",
    "    results = pd.DataFrame({'customer_id': customer_ids, 'prediction_default': predictions})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Loaded model is an instance of XGBClassifier.\n",
      "Parameters of the loaded model:\n",
      "{'objective': 'binary:logistic', 'use_label_encoder': False, 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.6775642138800437, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'gamma': None, 'gpu_id': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.14679275387962823, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 10, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 329, 'n_jobs': None, 'num_parallel_tree': None, 'predictor': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.9775566418243029, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n"
     ]
    }
   ],
   "source": [
    "# paths\n",
    "data_path = '../data/raw/test.csv'\n",
    "imputer_path = '../model-xgb/simple_imputer.pkl'\n",
    "scaler_path = '../model-xgb/min-max-scaler.pkl'\n",
    "xgb_model_path = '../model-xgb/best_xgb_model.pkl'\n",
    "original_features = ['age', 'gender', 'owns_car', 'owns_house', 'net_yearly_income',\n",
    "       'no_of_days_employed', 'occupation_type', 'total_family_members',\n",
    "       'migrant_worker', 'yearly_debt_payments', 'credit_limit_used_pctg',\n",
    "       'credit_score', 'total_defaults']\n",
    "\n",
    "# loading\n",
    "df = load_data(data_path)\n",
    "\n",
    "# loading assets\n",
    "model, imputer, scaler = load_assets(xgb_model_path, imputer_path, scaler_path)\n",
    "\n",
    "# checking if the model was serialized and deserialized correctly\n",
    "\n",
    "if isinstance(model, XGBClassifier):\n",
    "    print(\"Success! Loaded model is an instance of XGBClassifier.\")\n",
    "else:\n",
    "    print(\"Error! Loaded object is not an XGBClassifier.\")\n",
    "\n",
    "# Print the model's parameters (you should manually check if these are what you expect)\n",
    "print(\"Parameters of the loaded model:\")\n",
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning, engineering and preprocessing\n",
    "cleaned_data = clean_data(df)\n",
    "engineered_data = feature_engineering(cleaned_data)\n",
    "# getting ids and data\n",
    "X, customer_ids = formatting(engineered_data)\n",
    "# transforming data\n",
    "X = transform_data(X, imputer, scaler, original_features)\n",
    "# checking data quality for prediction\n",
    "X = prepare_for_prediction(X, model)\n",
    "# making predictions on production data\n",
    "results = make_predictions(X, model)\n",
    "\n",
    "results.to_csv('../reports/documentation//results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes_op",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
